{"title":"ML_assignment_by_Anshu_Kumar_Bishwas_202000284.pdf","contents":["MACHINE LEARNING ASSIGNMENT \nPROGRAM ELECTIVE IV \n \n \nNAME: Anshu Kumar Bishwas \nREGISTRATION NUMBER: 202000284 \n \nObjective: Prepare PPT/notes on K-Means Clustering and \nAgglomerative hierarchical Clustering Algorithm. Show \nexamples/working for each algorithm. \nK-Means Clustering: \n1. K-Means is an unsupervised clustering algorithm that aims to \npartition data into K clusters, where each data point belongs to \nthe cluster with the nearest mean. \n2. Algorithm:","the cluster with the nearest mean. \n2. Algorithm: \n• Initialize K cluster centroids randomly. \n• Assign each data point to the nearest centroid. \n• Recalculate the centroids as the mean of points in each \ncluster. \n• Repeat the assignment and centroid update steps until \nconvergence (when centroids no longer change \nsignificantly).","3. Example: \nStep 1: Initialize Centroids \nStep 2: Assign Points to Nearest Centroids \nPoint A is closer to Centroid 1, so it is assigned to Cluster 1. \nPoint B is closer to Centroid 1, so it is assigned to Cluster 1. \nPoint C is closer to Centroid 2, so it is assigned to Cluster 2. \nStep 3: Recalculate Centroids \nNew Centroid 1 is the average of points in Cluster 1: [(1, 2) + (4, \n5)] / 2 = (2.5, 3.5) New Centroid 2 is the average of points in \nCluster 2: (8, 8)","5)] / 2 = (2.5, 3.5) New Centroid 2 is the average of points in \nCluster 2: (8, 8) \nStep 4: Repeat until Convergence \nRepeat steps 2 and 3 until the centroids no longer change \nsignificantly. In this case, the centroids have converged, and the \nfinal clusters are: \nCluster 1: Points A and B \nCluster 2: Point C \n \n4. Applications: Image compression, customer segmentation, \nanomaly detection.","Agglomerative Hierarchical Clustering: \n \n1. Agglomerative hierarchical clustering is a bottom-up approach \nto clustering. It starts with individual data points as separate \nclusters and merges them until all points belong to a single \ncluster or a predefined number of clusters is reached. \n2. Algorithm: \n• Start with each data point as a separate cluster. \n• Merge the two closest clusters into a new cluster. \n• Repeat the merging process until the desired number of","• Repeat the merging process until the desired number of \nclusters is achieved or all points belong to a single cluster. \n3. Example: \nSteps: \n• Consider each alphabet as a single cluster and calculate the \ndistance of one cluster from all the other clusters. \n• In the second step, comparable clusters are merged together to \nform a single cluster. Let’s say cluster (B) and cluster (C) are \nvery similar to each other therefore we merge them in the","second step similarly to cluster (D) and (E) and at last, we get \nthe clusters [(A), (BC), (DE), (F)] \n• We recalculate the proximity according to the algorithm and \nmerge the two nearest clusters([(DE), (F)]) together to form \nnew clusters as [(A), (BC), (DEF)] \n• Repeating the same process; the clusters DEF and BC are \ncomparable and merged together to form a new cluster. We’re \nnow left with clusters [(A), (BCDEF)]. \n• At last, the two remaining clusters are merged together to form","• At last, the two remaining clusters are merged together to form \na single cluster [(ABCDEF)]. \n \n4. Applications: Phylogenetic, taxonomy, image segmentation. \n \n \nBoth K-Means and Agglomerative Hierarchical Clustering have their \nstrengths and weaknesses. K-Means is a partitional method that \nrequires specifying the number of clusters in advance, while \nAgglomerative Hierarchical Clustering builds a tree-like structure that \ndoesn't require specifying the number of clusters beforehand. The","doesn't require specifying the number of clusters beforehand. The \nchoice of algorithm depends on the nature of your data and the \nspecific goals of your analysis."]}